"""
Pandas æ ¸å¿ƒåŠŸèƒ½å­¦ä¹ é¡¹ç›®
è¿™ä¸ªé¡¹ç›®æ¶µç›–äº† Pandas çš„ä¸»è¦åŠŸèƒ½ï¼Œé€šè¿‡å®é™…æ¡ˆä¾‹å¸®åŠ©ä½ æŒæ¡æ•°æ®åˆ†ææŠ€èƒ½

å­¦ä¹ æ–¹æ³•ï¼šæ…¢ä¸‹æ¥ï¼
1. å…ˆå°†æ‰€æœ‰çš„ä»£ç çœ‹æ‡‚ï¼Œä¸æ˜ç™½çš„é—®aiã€‚åªéœ€è¦äº†è§£ç»™å‡ºçš„è¿ç”¨åœºæ™¯ï¼Œä¸ç”¨å»æ‰©å±•ï¼Œä¸ç„¶å®¹æ˜“é™·å…¥è¿·æƒ˜ã€‚
2. å¯¹è¦ç‚¹è¿›è¡Œé’ˆå¯¹æ€§çš„æ‰©å±•å­¦ä¹ ï¼Œç­›é€‰å‡ºè¦ç‚¹ï¼Œä¸ç”¨å¹¿æ³›æ€§çš„éƒ½å»æ‰©å±•å­¦ä¹ ï¼Œä¸ç„¶éš¾ç‚¹å¤ªå¤§å®¹æ˜“æ”¾å¼ƒã€‚
3. è¿ç”¨åˆ°å®è·µå½“ä¸­ï¼Œå…ˆå¯¹æ‰©å±•å­¦ä¹ åçš„è¦ç‚¹è¿›è¡Œå®è·µå­¦ä¹ ï¼Œå†æ ¹æ®å®è·µè¿›è¡Œä¸åŒç¨‹åº¦çš„æ‰©å±•å­¦ä¹ ã€‚
4. ä»å­¦ä¹ åˆ°å®è·µï¼Œå†ä»å®è·µä¸­å­¦ä¹ ï¼Œå¾ªç¯å¾€å¤ï¼Œå‘¨è€Œå¤å§‹ï¼Œæºæºä¸æ–­ã€‚
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta

print("=" * 60)
print("Pandas å­¦ä¹ é¡¹ç›® - æ•°æ®åˆ†æå®Œæ•´æµç¨‹")
print("=" * 60)

# ============================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šåˆ›å»º DataFrame å’Œ Series
# ============================================================
print("\nã€ç¬¬ä¸€éƒ¨åˆ†ï¼šæ•°æ®ç»“æ„åŸºç¡€ã€‘")
print("-" * 60)

# 1.1 åˆ›å»º Series
print("\n1. åˆ›å»º Seriesï¼ˆä¸€ç»´æ•°æ®ï¼‰ï¼š")
series_data = pd.Series([10, 20, 30, 40, 50],
                        index=['a', 'b', 'c', 'd', 'e'],
                        name='é”€å”®é¢')
print(series_data)

# 1.2 åˆ›å»º DataFrame - ä»å­—å…¸
print("\n2. åˆ›å»º DataFrameï¼ˆäºŒç»´è¡¨æ ¼ï¼‰ï¼š")
data = {
    'å§“å': ['å¼ ä¸‰', 'æå››', 'ç‹äº”', 'èµµå…­', 'é’±ä¸ƒ'],
    'å¹´é¾„': [25, 30, 35, 28, 32],
    'éƒ¨é—¨': ['æŠ€æœ¯', 'é”€å”®', 'æŠ€æœ¯', 'å¸‚åœº', 'é”€å”®'],
    'å·¥èµ„': [8000, 7000, 9500, 6500, 7500],
    'å…¥èŒæ—¥æœŸ': pd.date_range('2020-01-01', periods=5, freq='M')
}
df = pd.DataFrame(data)
print(df)

# ============================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šæ•°æ®æŸ¥çœ‹å’ŒåŸºæœ¬ä¿¡æ¯
# ============================================================
print("\n\nã€ç¬¬äºŒéƒ¨åˆ†ï¼šæ•°æ®æŸ¥çœ‹ã€‘")
print("-" * 60)

print("\n1. æŸ¥çœ‹å‰å‡ è¡Œï¼š")
print(df.head(3))

print("\n2. æŸ¥çœ‹åŸºæœ¬ä¿¡æ¯ï¼š")
print(df.info())

print("\n3. ç»Ÿè®¡æè¿°ï¼š")
print(df.describe())

print("\n4. æ•°æ®å½¢çŠ¶ï¼š", df.shape)
print("5. åˆ—åï¼š", df.columns.tolist())
print("6. æ•°æ®ç±»å‹ï¼š\n", df.dtypes)

# ============================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ•°æ®é€‰æ‹©å’Œç´¢å¼•
# ============================================================
print("\n\nã€ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ•°æ®é€‰æ‹©ã€‘")
print("-" * 60)

print("\n1. é€‰æ‹©å•åˆ—ï¼š")
print(df['å§“å'])

print("\n2. é€‰æ‹©å¤šåˆ—ï¼š")
print(df[['å§“å', 'å·¥èµ„']])

print("\n3. ä½¿ç”¨ loc æŒ‰æ ‡ç­¾é€‰æ‹©ï¼ˆè¡Œå’Œåˆ—ï¼‰ï¼š")
print(df.loc[0:2, ['å§“å', 'éƒ¨é—¨']])

print("\n4. ä½¿ç”¨ iloc æŒ‰ä½ç½®é€‰æ‹©ï¼š")
print(df.iloc[0:3, [0, 3]])

print("\n5. æ¡ä»¶ç­›é€‰ï¼ˆå·¥èµ„å¤§äº7000ï¼‰ï¼š")
print(df[df['å·¥èµ„'] > 7000])

print("\n6. å¤šæ¡ä»¶ç­›é€‰ï¼ˆæŠ€æœ¯éƒ¨é—¨ä¸”å¹´é¾„å°äº33ï¼‰ï¼š")
print(df[(df['éƒ¨é—¨'] == 'æŠ€æœ¯') & (df['å¹´é¾„'] < 33)])

# ============================================================
# ç¬¬å››éƒ¨åˆ†ï¼šæ•°æ®æ·»åŠ å’Œä¿®æ”¹
# ============================================================
print("\n\nã€ç¬¬å››éƒ¨åˆ†ï¼šæ•°æ®ä¿®æ”¹ã€‘")
print("-" * 60)

# å¤åˆ¶æ•°æ®ä»¥é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
df_modified = df.copy()

print("\n1. æ·»åŠ æ–°åˆ—ï¼ˆè®¡ç®—å¹´ç»ˆå¥–ï¼‰ï¼š")
df_modified['å¹´ç»ˆå¥–'] = df_modified['å·¥èµ„'] * 2
print(df_modified[['å§“å', 'å·¥èµ„', 'å¹´ç»ˆå¥–']])

print("\n2. ä¿®æ”¹ç‰¹å®šå€¼ï¼š")
df_modified.loc[0, 'å·¥èµ„'] = 8500
print(f"å¼ ä¸‰çš„æ–°å·¥èµ„ï¼š{df_modified.loc[0, 'å·¥èµ„']}")

print("\n3. ä½¿ç”¨å‡½æ•°ä¿®æ”¹åˆ—ï¼ˆå·¥èµ„å¢é•¿10%ï¼‰ï¼š")
df_modified['å·¥èµ„'] = df_modified['å·¥èµ„'].apply(lambda x: x * 1.1)
print(df_modified[['å§“å', 'å·¥èµ„']])

# ============================================================
# ç¬¬äº”éƒ¨åˆ†ï¼šæ•°æ®æ¸…æ´—
# ============================================================
print("\n\nã€ç¬¬äº”éƒ¨åˆ†ï¼šæ•°æ®æ¸…æ´—ã€‘")
print("-" * 60)

# åˆ›å»ºåŒ…å«ç¼ºå¤±å€¼çš„æ•°æ®
data_dirty = {
    'äº§å“': ['A', 'B', 'C', 'D', 'E', 'A'],
    'é”€é‡': [100, np.nan, 150, 200, np.nan, 120],
    'ä»·æ ¼': [10, 20, np.nan, 30, 25, 10],
    'è¯„åˆ†': [4.5, 3.8, 4.2, np.nan, 4.0, 4.7]
}
df_dirty = pd.DataFrame(data_dirty)

print("\n1. åŸå§‹æ•°æ®ï¼ˆå«ç¼ºå¤±å€¼ï¼‰ï¼š")
print(df_dirty)

print("\n2. æ£€æŸ¥ç¼ºå¤±å€¼ï¼š")
print(df_dirty.isnull().sum())

print("\n3. åˆ é™¤å«ç¼ºå¤±å€¼çš„è¡Œï¼š")
print(df_dirty.dropna())

print("\n4. å¡«å……ç¼ºå¤±å€¼ï¼ˆç”¨å¹³å‡å€¼ï¼‰ï¼š")
df_filled = df_dirty.copy()
df_filled['é”€é‡'].fillna(df_filled['é”€é‡'].mean(), inplace=True)
print(df_filled)

print("\n5. åˆ é™¤é‡å¤è¡Œï¼š")
print(f"é‡å¤è¡Œæ•°ï¼š{df_dirty.duplicated().sum()}")
df_unique = df_dirty.drop_duplicates(subset=['äº§å“'])
print(df_unique)

# ============================================================
# ç¬¬å…­éƒ¨åˆ†ï¼šæ•°æ®åˆ†ç»„å’Œèšåˆ
# ============================================================
print("\n\nã€ç¬¬å…­éƒ¨åˆ†ï¼šåˆ†ç»„èšåˆã€‘")
print("-" * 60)

print("\n1. æŒ‰éƒ¨é—¨åˆ†ç»„è®¡ç®—å¹³å‡å·¥èµ„ï¼š")
dept_avg = df.groupby('éƒ¨é—¨')['å·¥èµ„'].mean()
print(dept_avg)

print("\n2. å¤šåˆ—èšåˆï¼š")
dept_stats = df.groupby('éƒ¨é—¨').agg({
    'å·¥èµ„': ['mean', 'max', 'min'],
    'å¹´é¾„': 'mean'
})
print(dept_stats)

print("\n3. ç»Ÿè®¡æ¯ä¸ªéƒ¨é—¨çš„äººæ•°ï¼š")
print(df['éƒ¨é—¨'].value_counts())

# ============================================================
# ç¬¬ä¸ƒéƒ¨åˆ†ï¼šæ•°æ®æ’åº
# ============================================================
print("\n\nã€ç¬¬ä¸ƒéƒ¨åˆ†ï¼šæ•°æ®æ’åºã€‘")
print("-" * 60)

print("\n1. æŒ‰å·¥èµ„é™åºæ’åˆ—ï¼š")
print(df.sort_values('å·¥èµ„', ascending=False))

print("\n2. å¤šåˆ—æ’åºï¼ˆå…ˆæŒ‰éƒ¨é—¨ï¼Œå†æŒ‰å·¥èµ„ï¼‰ï¼š")
print(df.sort_values(['éƒ¨é—¨', 'å·¥èµ„'], ascending=[True, False]))

# ============================================================
# ç¬¬å…«éƒ¨åˆ†ï¼šæ•°æ®åˆå¹¶
# ============================================================
print("\n\nã€ç¬¬å…«éƒ¨åˆ†ï¼šæ•°æ®åˆå¹¶ã€‘")
print("-" * 60)

# åˆ›å»ºä¸¤ä¸ªç›¸å…³çš„ DataFrame
df1 = pd.DataFrame({
    'å‘˜å·¥ID': [1, 2, 3, 4],
    'å§“å': ['å¼ ä¸‰', 'æå››', 'ç‹äº”', 'èµµå…­'],
    'éƒ¨é—¨ID': [101, 102, 101, 103]
})

df2 = pd.DataFrame({
    'éƒ¨é—¨ID': [101, 102, 103],
    'éƒ¨é—¨åç§°': ['æŠ€æœ¯éƒ¨', 'é”€å”®éƒ¨', 'å¸‚åœºéƒ¨']
})

print("\n1. å‘˜å·¥è¡¨ï¼š")
print(df1)
print("\n2. éƒ¨é—¨è¡¨ï¼š")
print(df2)

print("\n3. åˆå¹¶ä¸¤ä¸ªè¡¨ï¼ˆJOINï¼‰ï¼š")
merged_df = pd.merge(df1, df2, on='éƒ¨é—¨ID', how='left')
print(merged_df)

print("\n4. çºµå‘è¿æ¥ï¼ˆconcatï¼‰ï¼š")
df3 = pd.DataFrame({
    'å‘˜å·¥ID': [5, 6],
    'å§“å': ['é’±ä¸ƒ', 'å­™å…«'],
    'éƒ¨é—¨ID': [102, 101]
})
concatenated = pd.concat([df1, df3], ignore_index=True)
print(concatenated)

# ============================================================
# ç¬¬ä¹éƒ¨åˆ†ï¼šæ•°æ®é€è§†è¡¨
# ============================================================
print("\n\nã€ç¬¬ä¹éƒ¨åˆ†ï¼šæ•°æ®é€è§†è¡¨ã€‘")
print("-" * 60)

# åˆ›å»ºé”€å”®æ•°æ®
sales_data = pd.DataFrame({
    'æ—¥æœŸ': pd.date_range('2024-01-01', periods=12, freq='M'),
    'åœ°åŒº': ['åŒ—äº¬', 'ä¸Šæµ·', 'åŒ—äº¬', 'ä¸Šæµ·', 'åŒ—äº¬', 'ä¸Šæµ·'] * 2,
    'äº§å“': ['A', 'A', 'B', 'B', 'A', 'A', 'B', 'B', 'A', 'A', 'B', 'B'],
    'é”€å”®é¢': [100, 150, 200, 180, 120, 160, 210, 190, 130, 170, 220, 200]
})

print("\n1. é”€å”®æ•°æ®ï¼š")
print(sales_data.head(8))

print("\n2. åˆ›å»ºé€è§†è¡¨ï¼ˆåœ°åŒº vs äº§å“ï¼‰ï¼š")
pivot = sales_data.pivot_table(
    values='é”€å”®é¢',
    index='åœ°åŒº',
    columns='äº§å“',
    aggfunc='sum'
)
print(pivot)

# ============================================================
# ç¬¬åéƒ¨åˆ†ï¼šæ—¶é—´åºåˆ—å¤„ç†
# ============================================================
print("\n\nã€ç¬¬åéƒ¨åˆ†ï¼šæ—¶é—´åºåˆ—ã€‘")
print("-" * 60)

# åˆ›å»ºæ—¶é—´åºåˆ—æ•°æ®
dates = pd.date_range('2024-01-01', periods=10, freq='D')
ts_data = pd.DataFrame({
    'æ—¥æœŸ': dates,
    'æ¸©åº¦': [15, 16, 14, 17, 18, 19, 20, 18, 17, 16]
})
ts_data.set_index('æ—¥æœŸ', inplace=True)

print("\n1. æ—¶é—´åºåˆ—æ•°æ®ï¼š")
print(ts_data)

print("\n2. æå–æ—¥æœŸç»„ä»¶ï¼š")
ts_data['æœˆä»½'] = ts_data.index.month
ts_data['æ˜ŸæœŸ'] = ts_data.index.dayofweek
print(ts_data)

print("\n3. é‡é‡‡æ ·ï¼ˆæŒ‰å‘¨æ±‚å¹³å‡ï¼‰ï¼š")
weekly_avg = ts_data['æ¸©åº¦'].resample('W').mean()
print(weekly_avg)

# ============================================================
# ç¬¬åä¸€éƒ¨åˆ†ï¼šå­—ç¬¦ä¸²æ“ä½œ
# ============================================================
print("\n\nã€ç¬¬åä¸€éƒ¨åˆ†ï¼šå­—ç¬¦ä¸²æ“ä½œã€‘")
print("-" * 60)

text_data = pd.DataFrame({
    'å§“å': ['  å¼ ä¸‰  ', 'LI Si', 'WANG Wu'],
    'é‚®ç®±': ['zhangsan@example.com', 'lisi@example.com', 'wangwu@example.com']
})

print("\n1. åŸå§‹æ•°æ®ï¼š")
print(text_data)

print("\n2. å­—ç¬¦ä¸²å¤„ç†ï¼š")
text_data['å§“å_æ¸…æ´—'] = text_data['å§“å'].str.strip().str.upper()
text_data['é‚®ç®±åŸŸå'] = text_data['é‚®ç®±'].str.split('@').str[1]
print(text_data)

# ============================================================
# ç¬¬åäºŒéƒ¨åˆ†ï¼šå¯¼å‡ºæ•°æ®
# ============================================================
print("\n\nã€ç¬¬åäºŒéƒ¨åˆ†ï¼šæ•°æ®å¯¼å‡ºã€‘")
print("-" * 60)

print("\nä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•å¯¼å‡ºæ•°æ®ï¼š")
print("1. df.to_csv('data.csv', index=False)  # å¯¼å‡ºä¸ºCSV")
print("2. df.to_excel('data.xlsx', index=False)  # å¯¼å‡ºä¸ºExcel")
print("3. df.to_json('data.json')  # å¯¼å‡ºä¸ºJSON")
print("4. df.to_sql('table_name', connection)  # å¯¼å‡ºåˆ°æ•°æ®åº“")

# ============================================================
# ç¬¬åä¸‰éƒ¨åˆ†ï¼šè¿›é˜¶ç»ƒä¹  - å¤æ‚æ•°æ®åˆ†æ
# ============================================================
print("\n\n" + "=" * 60)
print("ã€ç¬¬åä¸‰éƒ¨åˆ†ï¼šè¿›é˜¶æŒ‘æˆ˜ç»ƒä¹ ã€‘")
print("=" * 60)

# åˆ›å»ºå¤æ‚çš„ç”µå•†æ•°æ®é›†
np.random.seed(42)
n_orders = 1000

ecommerce_data = pd.DataFrame({
    'è®¢å•ID': range(1001, 1001 + n_orders),
    'ç”¨æˆ·ID': np.random.randint(1, 201, n_orders),
    'äº§å“ç±»åˆ«': np.random.choice(['ç”µå­äº§å“', 'æœè£…', 'é£Ÿå“', 'å®¶å±…', 'å›¾ä¹¦'], n_orders),
    'è®¢å•é‡‘é¢': np.random.uniform(50, 5000, n_orders).round(2),
    'è®¢å•æ—¥æœŸ': pd.date_range('2023-01-01', periods=n_orders, freq='8H'),
    'æ”¯ä»˜æ–¹å¼': np.random.choice(['æ”¯ä»˜å®', 'å¾®ä¿¡', 'ä¿¡ç”¨å¡', 'ç°é‡‘'], n_orders),
    'é…é€çŠ¶æ€': np.random.choice(['å·²é€è¾¾', 'é…é€ä¸­', 'å·²å–æ¶ˆ', 'é€€è´§'], n_orders, p=[0.7, 0.15, 0.1, 0.05]),
    'ç”¨æˆ·è¯„åˆ†': np.random.choice([1, 2, 3, 4, 5, np.nan], n_orders, p=[0.02, 0.03, 0.1, 0.3, 0.5, 0.05])
})

print("\nğŸ“Š ç”µå•†æ•°æ®é›†ï¼ˆå‰10è¡Œï¼‰ï¼š")
print(ecommerce_data.head(10))

# ============================================================
# ç»ƒä¹ 1ï¼šå®¢æˆ·ä»·å€¼åˆ†æï¼ˆRFMæ¨¡å‹ï¼‰
# ============================================================
print("\n\nã€ç»ƒä¹ 1ï¼šå®¢æˆ·ä»·å€¼åˆ†æ - RFMæ¨¡å‹ã€‘")
print("-" * 60)

# åªåˆ†æå·²é€è¾¾çš„è®¢å•
delivered_orders = ecommerce_data[ecommerce_data['é…é€çŠ¶æ€'] == 'å·²é€è¾¾'].copy()

# è®¡ç®—RFMæŒ‡æ ‡
reference_date = delivered_orders['è®¢å•æ—¥æœŸ'].max() + timedelta(days=1)

rfm = delivered_orders.groupby('ç”¨æˆ·ID').agg({
    'è®¢å•æ—¥æœŸ': lambda x: (reference_date - x.max()).days,  # Recency
    'è®¢å•ID': 'count',  # Frequency
    'è®¢å•é‡‘é¢': 'sum'  # Monetary
}).rename(columns={
    'è®¢å•æ—¥æœŸ': 'Recency',
    'è®¢å•ID': 'Frequency',
    'è®¢å•é‡‘é¢': 'Monetary'
})

# RFMåˆ†æ•°è®¡ç®—ï¼ˆäº”åˆ†ä½æ•°ï¼‰
rfm['R_Score'] = pd.qcut(rfm['Recency'], 5, labels=[5, 4, 3, 2, 1])
rfm['F_Score'] = pd.qcut(rfm['Frequency'].rank(method='first'), 5, labels=[1, 2, 3, 4, 5])
rfm['M_Score'] = pd.qcut(rfm['Monetary'], 5, labels=[1, 2, 3, 4, 5])

# ç»¼åˆRFMåˆ†æ•°
rfm['RFM_Score'] = rfm['R_Score'].astype(str) + rfm['F_Score'].astype(str) + rfm['M_Score'].astype(str)

# å®¢æˆ·åˆ†å±‚
def segment_customer(row):
    if row['R_Score'] >= 4 and row['F_Score'] >= 4 and row['M_Score'] >= 4:
        return 'é‡è¦ä»·å€¼å®¢æˆ·'
    elif row['R_Score'] >= 4 and row['F_Score'] >= 4:
        return 'é‡è¦å‘å±•å®¢æˆ·'
    elif row['R_Score'] >= 4:
        return 'é‡è¦ä¿æŒå®¢æˆ·'
    elif row['F_Score'] >= 4 and row['M_Score'] >= 4:
        return 'ä¸€èˆ¬ä»·å€¼å®¢æˆ·'
    else:
        return 'æ½œåœ¨å®¢æˆ·'

rfm['å®¢æˆ·åˆ†å±‚'] = rfm.apply(segment_customer, axis=1)

print("\n1. RFMåˆ†æç»“æœï¼ˆå‰10ä¸ªå®¢æˆ·ï¼‰ï¼š")
print(rfm.head(10))

print("\n2. å®¢æˆ·åˆ†å±‚ç»Ÿè®¡ï¼š")
print(rfm['å®¢æˆ·åˆ†å±‚'].value_counts())

print("\n3. å„å±‚çº§å®¢æˆ·å¹³å‡ä»·å€¼ï¼š")
segment_stats = rfm.groupby('å®¢æˆ·åˆ†å±‚').agg({
    'Monetary': 'mean',
    'Frequency': 'mean',
    'Recency': 'mean'
}).round(2)
print(segment_stats)

# ============================================================
# ç»ƒä¹ 2ï¼šæ»‘åŠ¨çª—å£åˆ†æå’ŒåŒæ¯”ç¯æ¯”
# ============================================================
print("\n\nã€ç»ƒä¹ 2ï¼šæ—¶é—´åºåˆ—æ»‘åŠ¨çª—å£åˆ†æã€‘")
print("-" * 60)

# æŒ‰æ—¥æœŸèšåˆé”€å”®æ•°æ®
daily_sales = ecommerce_data[ecommerce_data['é…é€çŠ¶æ€'] == 'å·²é€è¾¾'].groupby(
    ecommerce_data['è®¢å•æ—¥æœŸ'].dt.date
).agg({
    'è®¢å•é‡‘é¢': 'sum',
    'è®¢å•ID': 'count'
}).rename(columns={'è®¢å•é‡‘é¢': 'é”€å”®é¢', 'è®¢å•ID': 'è®¢å•é‡'})

# 7æ—¥ç§»åŠ¨å¹³å‡
daily_sales['é”€å”®é¢_7æ—¥å‡å€¼'] = daily_sales['é”€å”®é¢'].rolling(window=7).mean()
daily_sales['è®¢å•é‡_7æ—¥å‡å€¼'] = daily_sales['è®¢å•é‡'].rolling(window=7).mean()

# ç¯æ¯”å¢é•¿ç‡
daily_sales['é”€å”®é¢_ç¯æ¯”'] = daily_sales['é”€å”®é¢'].pct_change() * 100

# 7æ—¥å‰åŒæ¯”
daily_sales['é”€å”®é¢_7æ—¥åŒæ¯”'] = daily_sales['é”€å”®é¢'].pct_change(periods=7) * 100

print("\n1. æ—¶é—´åºåˆ—åˆ†æï¼ˆæœ€è¿‘10å¤©ï¼‰ï¼š")
print(daily_sales.tail(10).round(2))

print("\n2. é”€å”®è¶‹åŠ¿ç»Ÿè®¡ï¼š")
print(f"å¹³å‡æ—¥é”€å”®é¢: {daily_sales['é”€å”®é¢'].mean():.2f}")
print(f"æœ€é«˜æ—¥é”€å”®é¢: {daily_sales['é”€å”®é¢'].max():.2f}")
print(f"é”€å”®é¢æ ‡å‡†å·®: {daily_sales['é”€å”®é¢'].std():.2f}")
print(f"å¹³å‡ç¯æ¯”å¢é•¿ç‡: {daily_sales['é”€å”®é¢_ç¯æ¯”'].mean():.2f}%")

# ============================================================
# ç»ƒä¹ 3ï¼šå¤šç»´é€è§†å’Œäº¤å‰åˆ†æ
# ============================================================
print("\n\nã€ç»ƒä¹ 3ï¼šå¤šç»´æ•°æ®é€è§†åˆ†æã€‘")
print("-" * 60)

# æ·»åŠ æ—¶é—´ç»´åº¦
ecommerce_data['æœˆä»½'] = ecommerce_data['è®¢å•æ—¥æœŸ'].dt.to_period('M')
ecommerce_data['å­£åº¦'] = ecommerce_data['è®¢å•æ—¥æœŸ'].dt.to_period('Q')

# å¤æ‚é€è§†è¡¨ï¼šç±»åˆ« Ã— æ”¯ä»˜æ–¹å¼ Ã— é…é€çŠ¶æ€
pivot_complex = pd.pivot_table(
    ecommerce_data,
    values='è®¢å•é‡‘é¢',
    index=['äº§å“ç±»åˆ«', 'æ”¯ä»˜æ–¹å¼'],
    columns='é…é€çŠ¶æ€',
    aggfunc=['sum', 'count', 'mean'],
    fill_value=0
)

print("\n1. å¤šç»´é€è§†è¡¨ï¼ˆéƒ¨åˆ†ç»“æœï¼‰ï¼š")
print(pivot_complex.head(10))

# äº¤å‰è¡¨åˆ†æ
print("\n2. äº§å“ç±»åˆ« vs é…é€çŠ¶æ€äº¤å‰åˆ†æï¼š")
cross_tab = pd.crosstab(
    ecommerce_data['äº§å“ç±»åˆ«'],
    ecommerce_data['é…é€çŠ¶æ€'],
    values=ecommerce_data['è®¢å•é‡‘é¢'],
    aggfunc='sum',
    margins=True,
    margins_name='æ€»è®¡'
)
print(cross_tab)

# ============================================================
# ç»ƒä¹ 4ï¼šå¤æ‚çš„æ¡ä»¶ç­›é€‰å’Œæ•°æ®è½¬æ¢
# ============================================================
print("\n\nã€ç»ƒä¹ 4ï¼šå¤æ‚æ¡ä»¶ç­›é€‰å’Œç‰¹å¾å·¥ç¨‹ã€‘")
print("-" * 60)

# åˆ›å»ºå¤æ‚çš„æ¡ä»¶åˆ—
def classify_order(row):
    if row['è®¢å•é‡‘é¢'] > 2000 and row['ç”¨æˆ·è¯„åˆ†'] >= 4:
        return 'ä¼˜è´¨å¤§å•'
    elif row['è®¢å•é‡‘é¢'] > 2000:
        return 'å¤§å•å¾…æå‡'
    elif row['ç”¨æˆ·è¯„åˆ†'] >= 4:
        return 'ä¼˜è´¨å°å•'
    elif pd.isna(row['ç”¨æˆ·è¯„åˆ†']):
        return 'æœªè¯„ä»·'
    else:
        return 'ä¸€èˆ¬è®¢å•'

ecommerce_data['è®¢å•åˆ†ç±»'] = ecommerce_data.apply(classify_order, axis=1)

print("\n1. è®¢å•åˆ†ç±»ç»Ÿè®¡ï¼š")
print(ecommerce_data['è®¢å•åˆ†ç±»'].value_counts())

# å¤šæ¡ä»¶å¤åˆç­›é€‰
high_value_orders = ecommerce_data[
    (ecommerce_data['è®¢å•é‡‘é¢'] > ecommerce_data['è®¢å•é‡‘é¢'].quantile(0.75)) &
    (ecommerce_data['é…é€çŠ¶æ€'] == 'å·²é€è¾¾') &
    (ecommerce_data['ç”¨æˆ·è¯„åˆ†'] >= 4) &
    (ecommerce_data['è®¢å•æ—¥æœŸ'] >= '2023-06-01')
]

print(f"\n2. é«˜ä»·å€¼è®¢å•æ•°é‡: {len(high_value_orders)}")
print(f"   å æ€»è®¢å•æ¯”ä¾‹: {len(high_value_orders) / len(ecommerce_data) * 100:.2f}%")

# ä½¿ç”¨ transform è¿›è¡Œç»„å†…æ ‡å‡†åŒ–
ecommerce_data['é‡‘é¢_ç±»åˆ«æ ‡å‡†åŒ–'] = ecommerce_data.groupby('äº§å“ç±»åˆ«')['è®¢å•é‡‘é¢'].transform(
    lambda x: (x - x.mean()) / x.std()
)

print("\n3. æ ‡å‡†åŒ–åçš„æ•°æ®ç¤ºä¾‹ï¼š")
print(ecommerce_data[['äº§å“ç±»åˆ«', 'è®¢å•é‡‘é¢', 'é‡‘é¢_ç±»åˆ«æ ‡å‡†åŒ–']].head(10))

# ============================================================
# ç»ƒä¹ 5ï¼šçª—å£å‡½æ•°å’Œæ’ååˆ†æ
# ============================================================
print("\n\nã€ç»ƒä¹ 5ï¼šçª—å£å‡½æ•°å’Œæ’ååˆ†æã€‘")
print("-" * 60)

# æ¯ä¸ªç±»åˆ«ä¸­çš„è®¢å•é‡‘é¢æ’å
ecommerce_data['ç±»åˆ«å†…æ’å'] = ecommerce_data.groupby('äº§å“ç±»åˆ«')['è®¢å•é‡‘é¢'].rank(
    method='dense', ascending=False
)

# ç´¯è®¡æ±‚å’Œ
ecommerce_data_sorted = ecommerce_data.sort_values('è®¢å•æ—¥æœŸ')
ecommerce_data_sorted['ç´¯è®¡é”€å”®é¢'] = ecommerce_data_sorted['è®¢å•é‡‘é¢'].cumsum()

# æ‰¾å‡ºæ¯ä¸ªç±»åˆ«çš„Top 3è®¢å•
top3_by_category = ecommerce_data[
    ecommerce_data['ç±»åˆ«å†…æ’å'] <= 3
].sort_values(['äº§å“ç±»åˆ«', 'ç±»åˆ«å†…æ’å'])

print("\n1. å„ç±»åˆ«Top 3è®¢å•ï¼š")
print(top3_by_category[['äº§å“ç±»åˆ«', 'è®¢å•é‡‘é¢', 'ç±»åˆ«å†…æ’å']].head(15))

# è®¡ç®—ç§»åŠ¨ä¸­ä½æ•°
category_rolling = ecommerce_data.sort_values('è®¢å•æ—¥æœŸ').groupby('äº§å“ç±»åˆ«')['è®¢å•é‡‘é¢'].rolling(
    window=10, min_periods=1
).median().reset_index(drop=True)

print("\n2. ç§»åŠ¨ä¸­ä½æ•°è®¡ç®—å®Œæˆï¼ˆå‰10æ¡ï¼‰ï¼š")
print(category_rolling.head(10))

# ============================================================
# ç»ƒä¹ 6ï¼šæ•°æ®è´¨é‡åˆ†æå’Œå¼‚å¸¸æ£€æµ‹
# ============================================================
print("\n\nã€ç»ƒä¹ 6ï¼šæ•°æ®è´¨é‡å’Œå¼‚å¸¸æ£€æµ‹ã€‘")
print("-" * 60)

# ä½¿ç”¨ IQR æ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼
Q1 = ecommerce_data['è®¢å•é‡‘é¢'].quantile(0.25)
Q3 = ecommerce_data['è®¢å•é‡‘é¢'].quantile(0.75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = ecommerce_data[
    (ecommerce_data['è®¢å•é‡‘é¢'] < lower_bound) |
    (ecommerce_data['è®¢å•é‡‘é¢'] > upper_bound)
]

print(f"\n1. å¼‚å¸¸å€¼æ£€æµ‹ï¼ˆIQRæ–¹æ³•ï¼‰ï¼š")
print(f"   ä¸‹ç•Œ: {lower_bound:.2f}, ä¸Šç•Œ: {upper_bound:.2f}")
print(f"   æ£€æµ‹åˆ° {len(outliers)} ä¸ªå¼‚å¸¸è®¢å•")
print(f"   å¼‚å¸¸è®¢å•å æ¯”: {len(outliers) / len(ecommerce_data) * 100:.2f}%")

# æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
print("\n2. æ•°æ®å®Œæ•´æ€§åˆ†æï¼š")
missing_summary = pd.DataFrame({
    'ç¼ºå¤±æ•°é‡': ecommerce_data.isnull().sum(),
    'ç¼ºå¤±æ¯”ä¾‹': (ecommerce_data.isnull().sum() / len(ecommerce_data) * 100).round(2)
})
print(missing_summary[missing_summary['ç¼ºå¤±æ•°é‡'] > 0])

# é‡å¤å€¼æ£€æŸ¥
duplicates = ecommerce_data.duplicated(subset=['ç”¨æˆ·ID', 'è®¢å•æ—¥æœŸ', 'è®¢å•é‡‘é¢']).sum()
print(f"\n3. å¯èƒ½çš„é‡å¤è®¢å•: {duplicates}")

# ============================================================
# ç»ƒä¹ 7ï¼šé«˜çº§åˆ†ç»„å’Œèšåˆ
# ============================================================
print("\n\nã€ç»ƒä¹ 7ï¼šé«˜çº§åˆ†ç»„èšåˆæ“ä½œã€‘")
print("-" * 60)

# è‡ªå®šä¹‰èšåˆå‡½æ•°
def sales_range(x):
    return x.max() - x.min()

def top3_avg(x):
    return x.nlargest(3).mean()

# å¤šå±‚æ¬¡èšåˆ
advanced_agg = ecommerce_data.groupby(['äº§å“ç±»åˆ«', 'æ”¯ä»˜æ–¹å¼']).agg({
    'è®¢å•é‡‘é¢': [
        'sum',
        'mean',
        'count',
        ('range', sales_range),
        ('top3_avg', top3_avg)
    ],
    'ç”¨æˆ·è¯„åˆ†': lambda x: x.dropna().mean()
}).round(2)

print("\n1. é«˜çº§èšåˆåˆ†æç»“æœï¼š")
print(advanced_agg.head(10))

# ä½¿ç”¨ agg ç»“åˆ lambda
user_behavior = ecommerce_data.groupby('ç”¨æˆ·ID').agg(
    æ€»æ¶ˆè´¹=('è®¢å•é‡‘é¢', 'sum'),
    è®¢å•æ•°=('è®¢å•ID', 'count'),
    å¹³å‡å®¢å•ä»·=('è®¢å•é‡‘é¢', 'mean'),
    æ¶ˆè´¹æ ‡å‡†å·®=('è®¢å•é‡‘é¢', 'std'),
    æœ€è¿‘è´­ä¹°=('è®¢å•æ—¥æœŸ', 'max'),
    é¦–æ¬¡è´­ä¹°=('è®¢å•æ—¥æœŸ', 'min'),
    è´­ä¹°å¤©æ•°=('è®¢å•æ—¥æœŸ', lambda x: (x.max() - x.min()).days)
).round(2)

print("\n2. ç”¨æˆ·è¡Œä¸ºæ·±åº¦åˆ†æï¼ˆå‰10ä¸ªç”¨æˆ·ï¼‰ï¼š")
print(user_behavior.head(10))

# ============================================================
# æŒ‘æˆ˜ä»»åŠ¡æç¤º
# ============================================================
print("\n\n" + "=" * 60)
print("ğŸ¯ è¿›é˜¶æŒ‘æˆ˜ä»»åŠ¡å®Œæˆï¼")
print("=" * 60)
print("\nğŸ’ª ä½ å·²ç»æŒæ¡äº†ï¼š")
print("âœ“ RFMå®¢æˆ·ä»·å€¼åˆ†æ")
print("âœ“ æ—¶é—´åºåˆ—æ»‘åŠ¨çª—å£å’ŒåŒç¯æ¯”åˆ†æ")
print("âœ“ å¤šç»´æ•°æ®é€è§†å’Œäº¤å‰è¡¨")
print("âœ“ å¤æ‚æ¡ä»¶ç­›é€‰å’Œç‰¹å¾å·¥ç¨‹")
print("âœ“ çª—å£å‡½æ•°å’Œæ’åè®¡ç®—")
print("âœ“ å¼‚å¸¸æ£€æµ‹å’Œæ•°æ®è´¨é‡åˆ†æ")
print("âœ“ é«˜çº§åˆ†ç»„èšåˆæŠ€å·§")

print("\n\nğŸš€ è¿›é˜¶ç»ƒä¹ å»ºè®®ï¼š")
print("-" * 60)
print("1. å°è¯•ä½¿ç”¨çœŸå®æ•°æ®é›†ï¼ˆKaggleã€UCIç­‰ï¼‰")
print("2. å®ç°å®Œæ•´çš„æ•°æ®åˆ†ææŠ¥å‘Šï¼ˆEDAï¼‰")
print("3. ç»“åˆ matplotlib/seaborn è¿›è¡Œå¯è§†åŒ–")
print("4. å­¦ä¹  Pandas æ€§èƒ½ä¼˜åŒ–æŠ€å·§")
print("5. æ¢ç´¢ Pandas ä¸ SQL çš„ç»“åˆä½¿ç”¨")
print("6. æŒ‘æˆ˜å¤„ç†ç™¾ä¸‡çº§ä»¥ä¸Šçš„å¤§æ•°æ®é›†")
print("=" * 60)